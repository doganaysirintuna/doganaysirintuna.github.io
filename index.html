<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Doganay Sirintuna</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css">
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="#top">About</a></li>
					<li><a href="#experience">Experience</a></li>
					<li><a href="#education">Education</a></li>
					<li><a href="#projects">Publications</a></li>
					<li><a href="#honors">Honors & Awards</a></li>
					<li><a href="#skills">Skills</a></li>
				</ul>
			</nav>

		<!-- Home -->
			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-4 col-5-large col-12-medium">
							<div class="flip-circle-container" onclick="this.classList.toggle('flip-active')">
							  <div class="flip-circle-inner">
								<div class="flip-circle-front">
								  <img src="images/Doganay.jpeg" alt="Front Image" />
								</div>
								<div class="flip-circle-back">
								  <img src="images/Doganay1.jpg" alt="Back Image" />
								</div>
							  </div>
							</div>
						  </div>
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1><strong>Doganay Sirintuna</strong></h1>
							</header>
							<p class="about-text">
								Welcome!  I'm Doganay Sirintuna, a final-year Ph.D. student in the <a href="https://hri.iit.it">Human-Robot Interfaces and Interaction (HRI²) Lab</a> at the <a href="https://www.iit.it/it/home">Italian Institute of Technology</a>, under the supervision of <a href="https://scholar.google.com/citations?user=1hKOgRoAAAAJ&hl">Dr. Arash Ajoudani</a>. <br>
								<br>
								My research focuses on physical Human-Robot Interaction (pHRI), mobile manipulation, and robot learning, with the goal of enabling robots to perform contact-rich tasks in dynamic, unstructured real-world environments.	
								During my doctoral studies, I have had the opportunity to actively contribute to several European research projects, including <a href="https://ergolean.eu/">Ergo-Lean</a>, <a href="https://project-sophia.eu/">SOPHIA</a>, <a href="https://concertproject.eu/">CONCERT</a>, and <a href="https://tornado-horizon.eu/">TORNADO</a>, which explore advancing collaborative robotics in both industrial and healthcare domains.<br>
								<br>
								When I'm not playing with the robots, you'll likely find me traveling, staying active through sports, or supporting my favorite sports club, Beşiktaş.
							</p>
							<ul class="social">
								<li><a href="mailto:doganaysirintuna@gmail.com" class="fa fa-envelope" target="_blank" rel="noopener noreferrer" aria-label="Email"></a></li>
								<li><a href="https://www.linkedin.com/in/doganay-sirintuna/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://scholar.google.com/citations?user=ZCjO4IAAAAAJ&hl" class="ai ai-google-scholar"></a></li>
								<li><a href="https://www.researchgate.net/profile/Doganay-Sirintuna" class="ai ai-researchgate" target="_blank" rel="noopener noreferrer" aria-label="ResearchGate"></a></li>
							</ul>
						</div>
					</div>
				</div>
			</article>


			<!-- Experience  -->
			<hr class="m-0" />
			<section class="resume-section education-section" id="experience">
			  <div class="resume-section-content">
				<h2 class="mb-5">Experience</h2>
			
				<div class="timeline">
					<div class="timeline-item">
						<div class="card">
							<div class="text-block">
								<h3>Ph.D. Student in Advanced and Humanoid Robotics</h3>
								<div class="logo-row">
									<img src="images/iit.jpg" class="institution-logo" />
									<img src="images/unige.png" class="institution-logo" />
									<div class="institution">Italian Institute of Technology / University of Genoa</div>
								</div>
							</div>
							<div class="date-location-row">
								<span class="date"><i class="fas fa-calendar-alt"></i>Nov 2022 - Nov 2025 (Expected)</span>
								<span class="location"><i class="fas fa-map-marker-alt"></i> Genoa, IT</span>
							</div>
							<p class="description">
								<strong>Supervisor: </strong> <a href="https://scholar.google.com/citations?user=1hKOgRoAAAAJ&hl">Dr. Arash Ajoudani</a><br><br>
								My tentative PhD thesis title is <strong><em>Adaptive Approaches for Collaborative Mobile Manipulation</em></strong>, 
								where I investigate the development of adaptive and safety-aware strategies that enable mobile manipulators to seamlessly assist human partners in complex collaborative scenarios involving both direct physical interaction and shared object manipulation.
							</p>
						</div>
					</div>


			
					<div class="timeline-item">
						<div class="card">
							<div class="text-block">
								<h3>Research Fellow</h3>
								<div class="logo-row">
									<img src="images/iit.jpg" class="institution-logo" />
									<div class="institution">Italian Institute of Technology</div>
								</div>
							</div>
							<div class="date-location-row">
								<span class="date"><i class="fas fa-calendar-alt"></i>Jun 2021 - Sep 2022</span>
								<span class="location"><i class="fas fa-map-marker-alt"></i> Genoa, IT</span>
							</div>
							<p class="description">
								<strong>Supervisor: </strong> <a href="https://scholar.google.com/citations?user=1hKOgRoAAAAJ&hl">Dr. Arash Ajoudani</a><br><br>
								During this period, I explored the design of a deformation-agnostic framework to enable effective collaborative object transportation between a human and a mobile manipulator.


							</p>
						</div>
					</div>
			
					<div class="timeline-item">
						<div class="card">

							<div class="text-block">
								<h3>Research and Teaching Assistant</h3>
								<div class="logo-row">
									<img src="images/kocuni.jpg" class="institution-logo" />
									<div class="institution">Koc University</div>
								</div>
							</div>

							<div class="date-location-row">
								<span class="date"><i class="fas fa-calendar-alt"></i>Sep 2018 - Jul 2020</span>
								<span class="location"><i class="fas fa-map-marker-alt"></i> Istanbul, TR</span>
							</div>
							<p class="description">
							<strong>Supervisor: </strong> <a href="https://scholar.google.com/citations?user=Oz32f6gAAAAJ&hl"> Prof. Dr. Cagatay Basdogan</a><br><br>

							During my Master's studies, I actively contributed to a research project on human-robot collaboration funded by the Scientific and Technological Research Council of Turkey (TUBITAK), conducted at the <a href="https://rml.ku.edu.tr/">Robotics and Mechatronics Laboratory</a>. 
							As part of this work, I co-authored 2 journal papers and 2 conference proceedings. In addition to my research activities, I served as a teaching assistant for the following courses, where I led problem-solving sessions and laboratory exercises:<br><br>

							Courses: Robotics, Dynamic Modeling and Control, Computer-Based Modeling and Simulation, Manufacturing Processes, Mechanical Engineering Design Project	
							</p>
						</div>
					</div>

					
					<div class="timeline-item">
						<div class="card">

							<div class="text-block">
								<h3>Tutor</h3>
								<div class="logo-row">
									<img src="images/kocuni.jpg" class="institution-logo" />
									<div class="institution">Koc University</div>
								</div>
							</div>
							<div class="date-location-row">
								<span class="date"><i class="fas fa-calendar-alt"></i>2014 - 2018</span>
								<span class="location"><i class="fas fa-map-marker-alt"></i> Istanbul, TR</span>
							</div>
							<p class="description">
							During my Bachelor's studies, I worked as a tutor at Koç University's Office of Learning and Teaching, where I supported students in the following courses:<br><br>
							Introduction to Mechanical Engineering, Statics and Mechanics of Materials, Numerical Methods for Mechanical Engineering, Engineering Materials
							</p>
							</div>
						</div>
					</div>
			  </div>
			</section>


			<!-- Education  -->
			<hr class="m-0" />
			<section class="resume-section education-section" id="education">
			  <div class="resume-section-content">
				<h2 class="mb-5">Education</h2>
			
				<div class="timeline">

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h3>Ph.D. in Advanced and Humanoid Robotics</h3>
							<div class="logo-row">
								<img src="images/iit.jpg" class="institution-logo" />
								<img src="images/unige.png" class="institution-logo" />
								<div class="institution">Italian Institute of Technology / University of Genoa</div>
							</div>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>Nov 2022- Nov 2025</span>
							<span class="location"><i class="fas fa-map-marker-alt"></i> Genoa, IT</span>
						</div>


						<p class="description">
							<strong>Supervisor: </strong> <a href="https://scholar.google.com/citations?user=1hKOgRoAAAAJ&hl">Dr. Arash Ajoudani</a>
						</p>
						</div>
					</div>



				  <div class="timeline-item">
					<div class="card">

						<div class="text-block">
							<h3>M.Sc. in Robotics Engineering</h3>
							<div class="logo-row">
								<img src="images/kocuni.jpg" class="institution-logo" />
								<div class="institution">Koc University</div>
							</div>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>Sep 2018 - Jul 2020</span>
							<span class="location"><i class="fas fa-map-marker-alt"></i> Istanbul, TR</span>
						</div>
						<p class="description">
							Full-Merit Scholarship, GPA: 4.00<br>
							Graduate School of Sciences and Engineering Academic Excellence Award<br>
							Graduation with "Magna Cum Laude" (with high honor)<br>
							<strong>Supervisor: </strong> <a href="https://scholar.google.com/citations?user=Oz32f6gAAAAJ&hl"> Prof. Dr. Cagatay Basdogan</a>
						</p>
					</div>
				  </div>
			
				  <div class="timeline-item">

					<div class="card">

						<div class="text-block">
							<h3>B.Sc. in Mechanical Engineering</h3>
							<div class="logo-row">
								<img src="images/kocuni.jpg" class="institution-logo" />
								<div class="institution">Koc University</div>
							</div>
						</div>


						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>Sep 2013 - Jul 2018</span>
							<span class="location"><i class="fas fa-map-marker-alt"></i> Istanbul, TR</span>
						</div>
						<p class="description">
							Full-Merit Scholarship, GPA: 3.85<br>
							Ranked as 2nd in the class<br>
							Graduation with "Magna Cum Laude" (with high honor)<br>
						</p>
					</div>
				  </div>
				</div>
			  </div>
			</section>




		  <!-- Projects -->
		  <hr class="m-0" />
		  <section class="resume-section projects-section" id="projects">
		  <div class="resume-section-content">
			  <h2 class="mb-5">Publications</h2>
			  <h3>Journal Articles</h3>



			<!-- Work 1 -->
			<details close>
				<summary>Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author">Idil Ozdamar*, <strong>Doganay Sirintuna*</strong>, Robin Arbaud, and Arash Ajoudani</p><br>
				<p>Tactile-driven reactive pushing strategy for real-time mobile manipulation without prior knowledge of object, robot, or environment properties. 
					This method dynamically adjusts robot motion based on the object’s contact location, enabling effective manipulation in unstructured, real-world environments.</p>				  
					<div style="display: flex; align-items: center; gap: 16px;">
						<a href="https://ieeexplore.ieee.org/document/10556715" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
						text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
							<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
						</a>
						<p class="journal" style="margin: 0;"><em>IEEE Robotics and Automation Letters</em>, 2024</p>
					</div>
				</div>
				<div class="video-wrapper">
					<iframe 
					src="https://www.youtube.com/embed/3-zXarpVezA" 
					title="YouTube video player" 
					frameborder="0" 
					allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
					referrerpolicy="strict-origin-when-cross-origin"
					allowfullscreen>
					</iframe>
				</div>
				</div>
			</details>


			<!-- Work 2 -->
			<details close>
				<summary>Enhancing human-robot collaborative transportation through obstacle-aware vibrotactile warning and virtual fixtures
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author"><strong>Doganay Sirintuna*</strong>, Theodora Kastritsi*, Idil Ozdamar*, Juan M Gandarias, and Arash Ajoudani</p><br>
				<p>Situational awareness framework with vibrotactile feedback and virtual fixtures for safe human-robot collaborative transportation. 
					By delivering real-time obstacle warnings and enforcing mobility constraints, it helps operators navigate complex environments more safely and efficiently.</p>
					<div style="display: flex; align-items: center; gap: 16px;">
						<a href="https://www.sciencedirect.com/science/article/pii/S0921889024001088" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
						text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
							<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
						</a>
						<p class="journal" style="margin: 0;"><em>Robotics and Autonomous Systems</em>, 2024</p>
					</div>
				</div>
				<div class="video-wrapper">
					<iframe 
					src="https://www.youtube.com/embed/BRvDC8XLOWM" 
					title="YouTube video player" 
					frameborder="0" 
					allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
					referrerpolicy="strict-origin-when-cross-origin"
					allowfullscreen>
					</iframe>
				</div>
				</div>
			</details>


			<!-- Work 3 -->
			<details close>
				<summary>An Object Deformation-Agnostic Framework for Human-Robot Collaborative Transportation
				</summary>
				<div class="two-column">
				<div class="explanation">
					<p class="author"><strong>Doganay Sirintuna</strong>, Alberto Giammarino, and Arash Ajoudani</p><br>
					<p>Adaptive object deformability-agnostic human-robot collaborative transportation framework that combines the haptic information transferred through the object with the human kinematic information to generate reactive whole-body motions on a mobile collaborative robot. 
					Furthermore, it allows rotating the objects based on an algorithm that detects the human rotation intention using the torso and hand movements.</p>
					<div style="display: flex; align-items: center; gap: 16px;">
						<a href="https://ieeexplore.ieee.org/abstract/document/10081043" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
						text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
							<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
						</a>
						<p class="journal" style="margin: 0;"><em>IEEE Transactions on Automation Science and Engineering</em>, 2022</p>
					</div>
				</div>
				<div class="video-wrapper">
					<iframe 
					src="https://www.youtube.com/embed/oyfUkYj5WYw" 
					title="YouTube video player" 
					frameborder="0" 
					allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
					referrerpolicy="strict-origin-when-cross-origin"
					allowfullscreen>
					</iframe>
				</div>
				</div>
			</details>


			<h3>Conference Proceedings</h3>

			<!-- Work 1 -->
			<details close>
				<summary>Robot-assisted navigation for visually impaired through adaptive impedance and path planning
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author">Pietro Balatti*, Idil Ozdamar*, <strong>Doganay Sirintuna*</strong>, Luca Fortini, Mattia Leonori, Juan M Gandarias, and Arash Ajoudani</p><br>
				<p>Framework for guiding visually impaired individuals through unfamiliar environments using a mobile manipulator with coordinated obstacle avoidance and human guidance. 
				The system employs LiDAR-based leg tracking and adaptive pulling, using impedance control to dynamically adjust the robotic arm’s movements and guide the user back to the intended path for safe and intuitive navigation.</p>
				<div style="display: flex; align-items: center; gap: 16px;">
					<a href="https://ieeexplore.ieee.org/abstract/document/10611071" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
					text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
						<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
					</a>
					<p class="journal" style="margin: 0;"><em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024</p>
				</div>
			</div>
			<div class="video-wrapper">
				<iframe 
				src="https://www.youtube.com/embed/B94n3QjdnJE" 
				title="YouTube video player" 
				frameborder="0" 
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
				referrerpolicy="strict-origin-when-cross-origin"
				allowfullscreen>
				</iframe>
			</div>
				</div>
			</details>



			<!-- Work 2 -->
			<details close>
				<summary>Carrying the uncarriable: a deformation-agnostic and human-cooperative framework for unwieldy objects using multiple robots
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author"><strong>Doganay Sirintuna</strong>, Idil Ozdamar, and Arash Ajoudani</p> <br>
				<p>Object deformability-agnostic framework for co-carrying tasks shared between a person and multiple robots. 
				The approach allows full control of the co-carrying trajectories by the person using haptic feedback from the object combined with human motion information..</p>
				<div style="display: flex; align-items: center; gap: 16px;">
					<a href="https://ieeexplore.ieee.org/abstract/document/10160677" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
					text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
						<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
					</a>
					<p class="journal" style="margin: 0;"><em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023</p>
				</div>
			</div>
			<div class="video-wrapper">
				<iframe src="https://www.youtube.com/embed/Q3sA6YzTaaE" 
				title="YouTube video player" 
				frameborder="0" 
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
				referrerpolicy="strict-origin-when-cross-origin" 
				allowfullscreen>
				</iframe>
			</div>
			</div>
			</details>
			

			<!-- Work 3 -->
			<details close>
				<summary>Human-robot collaborative carrying of objects with unknown deformation characteristics
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author"><strong>Doganay Sirintuna</strong>, Alberto Giammarino, and Arash Ajoudani</p> <br>
				<p>Adaptive control framework for human–robot collaborative transportation of objects with unknown deformation behavior. 
					The system combines haptic and human motion information to generate reactive whole-body robot motions, enabling intuitive and effective co-transportation across varying object deformabilities..</p>
				<div style="display: flex; align-items: center; gap: 16px;">
					<a href="https://ieeexplore.ieee.org/abstract/document/9981948" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
					text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
						<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
					</a>
					<p class="journal" style="margin: 0;"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022</p>
				</div>
			</div>
			<div class="video-wrapper">
				<iframe src="https://www.youtube.com/embed/ounQmCdYlC4" 
				title="YouTube video player" 
				frameborder="0" 
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
				referrerpolicy="strict-origin-when-cross-origin" 
				allowfullscreen>
				</iframe>
			</div>
			</div>
			</details>


			<!-- Work 4 -->
			<details close>
				<summary>Detecting human motion intention during pHRI using artificial neural networks trained by EMG signals
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author"><strong>Doganay Sirintuna*</strong>, Idil Ozdamar*, Yusuf Aydin, and Cagatay Basdogan</p> <br>
				<p>Intention-aware admittance control architecture for physical human-robot interaction using EMG-based direction classification. 
				An artificial neural network predicts human motion intent from arm muscle signals and modulates the cobot's admittance controller in real time, reducing undesired movements without increasing task completion time.</p>
				<div style="display: flex; align-items: center; gap: 16px;">
					<a href="https://ieeexplore.ieee.org/abstract/document/9223438" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
					text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
						<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
					</a>
					<p class="journal" style="margin: 0;"><em>IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2020</p>
				</div>
			</div>
			<div class="video-wrapper">
				<iframe src="https://www.youtube.com/embed/8F1HCEKEr4s" 
				title="YouTube video player" 
				frameborder="0" 
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
				referrerpolicy="strict-origin-when-cross-origin" 
				allowfullscreen>
				</iframe>
			</div>
			</div>
			</details>


			<!-- Work 5 -->
			<details close>
				<summary>A variable-fractional order admittance controller for pHRI
				</summary>
				<div class="two-column">
				<div class="explanation">
				<p class="author"><strong>Doganay Sirintuna*</strong>, Yusuf Aydin*, Ozan Caldiran, Ozan Tokatli, Volkan Patoglu, and Cagatay Basdogan</p> <br>
				<p>Fractional-order variable admittance controller for transparent and stable physical human-robot interaction in manufacturing tasks. 
				Compared to baseline controllers, the proposed method improves transparency without compromising stability, and integrates with augmented reality to enhance human sensory feedback during precision operations like drilling.</p>
				<div style="display: flex; align-items: center; gap: 16px;">
					<a href="https://ieeexplore.ieee.org/abstract/document/9197288" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
					text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
						<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
					</a>
					<p class="journal" style="margin: 0;"><em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020</p>
				</div>
			</div>
			<div class="video-wrapper">
				<iframe src="https://www.youtube.com/embed/VvPMVEHXBMk" 
				title="YouTube video player" 
				frameborder="0" 
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
				referrerpolicy="strict-origin-when-cross-origin" 
				allowfullscreen>
				</iframe>
			</div>
			</div>
			</details>
			


			<h3>Contributed Works</h3>


			<!-- Work 1 -->
			<details close>
				<summary>A Non-parametric Approach to Exploring and Quantifying the Information Flow in Human-Robot Collaboration
				</summary>
				<div class="two-column">
				<div class="explanation">
					<p class="author">Gustavo Jose Giardini Lahr, <strong>Doganay Sirintuna</strong>, Francesco Tassi, Heni Ben Amor, Arash Ajoudani</p><br>
					<p>Information-theoretic framework for analyzing non-verbal communication in physical human-robot interaction. 
					Using entropy-based measures, the approach quantifies information flow, leadership, and coupling between agents, revealing key communication dynamics in collaborative tasks.</p>
					<div style="display: flex; align-items: center; gap: 16px;">
						<a href="https://dl.acm.org/doi/10.1145/3744755" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
						text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
							<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
						</a>
						<p class="journal" style="margin: 0;"><em>ACM Transactions on Human-Robot Interaction</em>, 2024</p>
					</div>
				</div>
				<div class="video-wrapper">
					<img src="images/Gustavo_paper.jpg" alt="Project preview" style="width: 100%; border-radius: 8px;">
				</div>
				</div>
			</details>

			<!-- Work 2 -->
			<details close>
				<summary>Evaluating leadership roles in human-robot interaction via highly dynamic collaborative tasks
				</summary>
				<div class="two-column">
				<div class="explanation">
					<p class="author">Gustavo Jose Giardini Lahr, Francesco Tassi, <strong>Doganay Sirintuna</strong>, Heni Ben Amor, and Arash Ajoudani</p><br>
					<p>Study of non-verbal communication and leadership dynamics in human-robot interaction through a collaborative object-catching task. 
					Comparison between human-human and human-robot interactions across control modalities reveals the influence of leadership roles in highly dynamic tasks.</p>
					<div style="display: flex; align-items: center; gap: 16px;">
						<a href="https://ieeexplore.ieee.org/abstract/document/10731284" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
						text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
							<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
						</a>
						<p class="journal" style="margin: 0;"><em>IEEE International Conference on Robot and Human Interactive Communication (ROMAN)</em>, 2024</p>
					</div>
				</div>
				<div class="video-wrapper">
					<img src="images/Tassi_paper.jpg" alt="Project preview" style="width: 100%; border-radius: 3px; display: block; margin: 0 auto;">
				</div>
				</div>
			</details>

			<!-- Work 3 -->
			<details close>
				<summary>A Novel Haptic Feature Set for the Classification of Interactive Motor Behaviors in Collaborative Object Transfer
				</summary>
				<div class="two-column">
				<div class="explanation">
					<p class="author">Zaid Al-Saadi, <strong>Doganay Sirintuna</strong>, Ayse Kucukyilmaz, and Cagatay Basdogan</p><br>
						<p>Haptic-based classification of interaction patterns in physical human–human collaboration during joint object transportation. 
						Using force and torque data, the study identifies cooperative and conflicting behaviors during translational and rotational movements.</p>
						<div style="display: flex; align-items: center; gap: 16px;">
							<a href="https://ieeexplore.ieee.org/abstract/document/9241412" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
							text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
								<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
							</a>
							<p class="journal" style="margin: 0;"><em>IEEE Transactions on Haptics</em>, 2020</p>
						</div>
					</div>
					<div class="video-wrapper">
						<img src="images/Zaid.png" alt="Project preview" style="width: 100%; border-radius: 8px; display: block; margin: 0 auto;">
					</div>
				</div>
			</details>


			<!-- Work 4 -->
			<details close>
				<summary>Towards collaborative drilling with a cobot using admittance controller
				</summary>
				<div class="two-column">
				<div class="explanation">
					<p class="author">Yusuf Aydin, <strong>Doganay Sirintuna</strong>, and Cagatay Basdogan</p><br>
						<p>Methodology for designing admittance controllers balancing stability and transparency in physical human-robot interaction. 
						Applied to a collaborative drilling task with a KUKA LBR IIWA 7 R800 cobot and augmented reality interface, the approach identifies optimal controller parameters to enhance task performance and operator guidance.</p>
						<div style="display: flex; align-items: center; gap: 16px;">
							<a href="https://journals.sagepub.com/doi/abs/10.1177/0142331220934643" target="_blank" style="display: inline-flex; align-items: center; justify-content: center; background-color: #585452; color: white; padding: 6px 12px; border-radius: 12px; 
							text-decoration: none; font-weight: 600; font-size: 13px; gap: 10px;">
								<i class="fas fa-file-alt" style="margin-right: 8px;"></i> Paper link
							</a>
							<p class="journal" style="margin: 0;"><em>Transactions of the Institute of Measurement and Control</em>, 2020</p>
						</div>
					</div>
					<div class="video-wrapper">
						<img src="images/Yusuf.jpg" alt="Project preview" style="width: 100%; border-radius: 8px; display: block; margin: 0 auto;">
					</div>
				</div>
			</details>

		</section>

		<!-- Honors and Awards  -->
		<hr class="m-0" />
		<section class="resume-section education-section" id="honors">
			<div class="resume-section-content">
			<h2 class="mb-5">Honors & Awards</h2>
			<div class="timeline">

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Finalist for the Best Paper Award on Human-Robot Interaction at the ICRA 2024</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2024</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Recipient of the Academic Excellence Award from the Graduate School of Sciences and Engineering, Koc University</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2020</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Graduated 2<sup>nd</sup> in the Department of Mechanical Engineering, Koc University</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2018</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Honor of Vehbi Koç Scholar (4-time recipient), Koc University</h5>
						</div>
						<p class="description">
							Awarded for exceptional academic success at Koç University during undergraduate studies
						</p>

						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2015 - 2018</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Ranked within the top 1% in Turkey's Selection Exam for Academic Personnel and Graduate Studies (ALES)</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2017</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Ranked within the top 0.1% in National University Entrance Exam</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2013</span>
						</div>
					</div>
				</div>


				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Honorable Mention in Akdeniz University Math Olympics</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2012</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Recipient of 2 Bronze Medals in Tubitak Primary Education Math Olympics</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2008 - 2009</span>
						</div>
					</div>
				</div>

				<div class="timeline-item">
					<div class="card">
						<div class="text-block">
							<h5>Outstanding Success Award in American Mathematics Contest</h5>
						</div>
						<div class="date-location-row">
							<span class="date"><i class="fas fa-calendar-alt"></i>2008</span>
						</div>
					</div>
				</div>
			</div>
			</div>
			
		</section>

		<!-- Skills -->

		<hr class="m-0" />
		
		<section class="resume-section education-section" id="skills">
			<div class="resume-section-content">
			<h2 class="mb-5">Skills</h2>

			  <div class="skills-grid">
				<!-- Programming -->
				<div class="skill-card">
				  <h3>Programming</h3>
				  <ul>
					<li><img src="images/c++.png" class="custom-icon" alt="C++"> C++</li>
					<li><img src="images/python.png" class="custom-icon" alt="Python">Python</li>
					<li><img src="images/Matlab.png" class="custom-icon" alt="Matlab">MATLAB</li>
					<li><img src="images/c-sharp.png" class="custom-icon" alt="C#">C#</li>
				  </ul>
				</div>
		  
				<!-- Robotics -->
				<div class="skill-card">
				  <h3>Robotics</h3>
				  <ul>
					<li><img src="images/Ros_logo.svg" class="custom-icon" alt="ROS"> ROS</li>
					<li><img src="images/frankarobotics_logo.jpeg" class="custom-icon" alt="Franka"> Franka Control Interface</li>
					<li><img src="images/kuka_logo.png" class="custom-icon" alt="Kuka"> KUKA Sunrise</li>
					<li><img src="images/Universal_robots_logo.svg" class="custom-icon" alt="Universal_robots_logo"> URScript</li>

				  </ul>
				</div>
		  
				<!-- Tools & Libraries -->
				<div class="skill-card tools-libs">
					<h3>Tools & Libraries</h3>
				  <ul>
					<li><img src="images/Gazebo.svg" class="custom-icon" alt="Gazebo"> Gazebo</li>
					<li><img src="images/rviz.png" class="custom-icon" alt="Rviz"> Rviz</li>
					<li><img src="images/docker.png" class="custom-icon" alt="Docker"> Docker</li>
					<li><img src="images/latex.svg" class="custom-icon" alt="LaTeX"> LaTeX</li>
					<li><img src="images/git.png" class="custom-icon" alt="Git"> Git</li>
					<li><img src="images/Simulink.png" class="custom-icon" alt="Simulink"> Simulink</li>
					<li><img src="images/OpenCV.png" class="custom-icon" alt="OpenCV"> OpenCV</li>
					<li><img src="images/TensorFlow.svg" class="custom-icon" alt="TensorFlow"> TensorFlow</li>
					<li><img src="images/PyTorch.svg" class="custom-icon" alt="PyTorch">PyTorch</li>
					<li><img src="images/Scikit_learn.svg" class="custom-icon" alt="Scikit-Learn">Scikit-Learn</li>
					<li><img src="images/arduino.png" class="custom-icon" alt="Arduino">Arduino</li>
					<li><img src="images/nx-cad-cam.svg" class="custom-icon" alt="Siemens NX"> Siemens NX</li>
					<li><img src="images/solidworks.png" class="custom-icon" alt="Solidworks">Solidworks</li>
					<li><img src="images/unity.png" class="custom-icon" alt="Unity">Unity</li>
					<li><img src="images/hololens.png" class="custom-icon" alt="hololens">HoloLens</li>

				  </ul>
				</div>
		  

			  </div>
			</div>
		  </section>
		  

			
		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script>
				let hasFlipped = false;
			  
				// Basic mobile detection
				function isMobile() {
				  return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
				}
			  
				if (isMobile()) {
				  const flipContainers = document.querySelectorAll('.flip-circle-container');
			  
				  // Flip all containers once on first global tap/click
				  function autoFlipOnce() {
					if (hasFlipped) return;
			  
					flipContainers.forEach(container => container.classList.add('flip-active'));
					hasFlipped = true;
			  
					// Remove listeners after first flip
					document.removeEventListener('click', autoFlipOnce);
					document.removeEventListener('touchstart', autoFlipOnce);
				  }
			  
				  document.addEventListener('click', autoFlipOnce);
				  document.addEventListener('touchstart', autoFlipOnce);
			  
				  // Add toggle flip on tap for each container individually
				  flipContainers.forEach(container => {
					container.addEventListener('click', (e) => {
					  e.stopPropagation(); // Prevent global listener triggering again
					  container.classList.toggle('flip-active');
					});
				  });
				}
			  </script>

	</body>
</html>